{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your use case deals with creating summaries, you must ensure that your GenAI app produces \"good\" summaries:\n",
    "- Summaries that are factually aligned with the original text\n",
    "- Summaries that include important information from the original text\n",
    "\n",
    "We want to calculate how good the created summary is. Using **Question Answer Generation (QAG) Score** covered previously, we can calculate both factual alignment and inclusion scores to compute a final summarization score. The 'inclusion score' is calculated as the percentage of assessment questions for which both the summary and the original document provide a 'yes' answer. This method ensures that the summary not only includes key information from the original text but also accurately represents it. A higher inclusion score indicates a more comprehensive and faithful summary, signifying that the summary effectively encapsulates the crucial points and details from the original content.\n",
    "\n",
    "We will be using AWS Bedrock + Anthropic Claude 3.0 Sonnet model as our *LLM-as-a-judge*. Let's prepare and define the necessary parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepeval\n",
    "!pip install python-dotenv\n",
    "!pip install instructor\n",
    "!pip install \"anthropic[bedrock]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store all API keys and credentials in `.env` file. Load them now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure that we are authenticated and can call our Amazon Bedrock service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "from anthropic import AnthropicBedrock\n",
    "\n",
    "client = AnthropicBedrock()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Hey, how are you?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Amazon Bedrock Claude 3.0 Sonnet model as a judge LLM within `deepeval` framework, we need to implement a custom LLM class. Also, since we are defining a custom LLM class, we need to ensure that it responds in a properly structured JSON format. We will use [`instructor`](https://python.useinstructor.com/) python library to enforce structured LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vbaklikov/miniconda3/envs/dev/lib/python3.12/site-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.4.6, however version 1.4.8 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deepeval\n",
    "import instructor\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from pydantic import BaseModel\n",
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "\n",
    "class AWSBedrock(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = AnthropicBedrock()\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        chat_model = self.load_model()\n",
    "        instructor_client = instructor.from_anthropic(chat_model)\n",
    "        response = instructor_client.messages.create(\n",
    "            model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a world class AI that excels at extracting data from a sentence\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"AWS Bedrock Claude Sonnet 3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Summarization Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using AWS Bedrock Claude Sonnet </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.0</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mSummarization Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing AWS Bedrock Claude Sonnet \u001b[0m\u001b[1;38;2;55;65;81m3.0\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:26, 26.30s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Summarization (score: 0.0, threshold: 0.7, strict: False, evaluation model: AWS Bedrock Claude Sonnet 3.0, reason: The score is 0.00 because there is no original text provided to summarize from, so the summary cannot be evaluated for accuracy or completeness. However, since no contradictions or extra information are listed, the summary has not introduced any obvious errors. The inability to answer certain questions likely stems from the lack of content in the original text rather than a flaw in the summary itself., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Some long and boring paragraph that needs to be summarized by the LLM for the purpsoes of a test. Let's create a negative example where Input text is not summarized correctly\n",
      "  - actual output: A completely wrong summary\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Summarization: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved in data/ as 20241104_134128\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved in data/ as 20241104_134128\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(success=False, metrics_data=[MetricData(name='Summarization', threshold=0.7, success=False, score=0.0, reason='The score is 0.00 because there is no original text provided to summarize from, so the summary cannot be evaluated for accuracy or completeness. However, since no contradictions or extra information are listed, the summary has not introduced any obvious errors. The inability to answer certain questions likely stems from the lack of content in the original text rather than a flaw in the summary itself.', strict_mode=False, evaluation_model='AWS Bedrock Claude Sonnet 3.0', error=None, evaluation_cost=None, verbose_logs='Truths (limit=None):\\n[\\n    \"The given text does not contain any factual statements.\"\\n] \\n \\nClaims:\\n[\\n    \"A completely wrong summary\"\\n] \\n \\nAssessment Questions:\\n[\\n    \"Does the given text contain the phrase \\'long and boring paragraph\\'?\",\\n    \"Is the text intended to be summarized by an LLM?\",\\n    \"Does the text mention creating a negative example?\",\\n    \"Is the input text not summarized correctly?\",\\n    \"Does the paragraph discuss summarizing text for the purposes of a test?\"\\n] \\n \\nCoverage Verdicts:\\n[\\n    {\\n        \"summary_verdict\": \"no\",\\n        \"original_verdict\": \"yes\",\\n        \"question\": \"Does the given text contain the phrase \\'long and boring paragraph\\'?\"\\n    },\\n    {\\n        \"summary_verdict\": \"yes\",\\n        \"original_verdict\": \"yes\",\\n        \"question\": \"Is the text intended to be summarized by an LLM?\"\\n    },\\n    {\\n        \"summary_verdict\": \"yes\",\\n        \"original_verdict\": \"yes\",\\n        \"question\": \"Does the text mention creating a negative example?\"\\n    },\\n    {\\n        \"summary_verdict\": \"yes\",\\n        \"original_verdict\": \"yes\",\\n        \"question\": \"Is the input text not summarized correctly?\"\\n    },\\n    {\\n        \"summary_verdict\": \"yes\",\\n        \"original_verdict\": \"yes\",\\n        \"question\": \"Does the paragraph discuss summarizing text for the purposes of a test?\"\\n    }\\n] \\n \\nAlignment Verdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": \"The original text does not contain any factual information, so it is impossible to determine if the summary claim contradicts it or not.\"\\n    }\\n]')], conversational=False, multimodal=False, input=\"Some long and boring paragraph that needs to be summarized by the LLM for the purpsoes of a test. Let's create a negative example where Input text is not summarized correctly\", actual_output='A completely wrong summary', expected_output=None, context=None, retrieval_context=None)], confident_link=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase,LLMTestCaseParams\n",
    "from deepeval import evaluate\n",
    "\n",
    "custom_llm = AWSBedrock()\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Some long and boring paragraph that needs to be summarized by the LLM for the purpsoes of a test. Let's create a negative example where Input text is not summarized correctly\",\n",
    "    actual_output=\"A completely wrong summary\",\n",
    ")\n",
    "\n",
    "summarization_metric = SummarizationMetric(\n",
    "    model=custom_llm,\n",
    "    threshold=0.7,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "evaluate([test_case],[summarization_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As we can see, our LLM-as-a-judge assigns a correct score of 0.0 to the above generated summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
