# LLM Evaluation Metrics

Evaluating the outputs of Large Language Models (LLMs) is crucial for delivering reliable LLM applications, yet it remains a challenging task for many. Whether youâ€™re improving model accuracy through fine-tuning or optimizing contextual relevance in a Retrieval-Augmented Generation (RAG) system, establishing and selecting the right evaluation metrics for your specific use case is essential for building a robust LLM evaluation pipeline.

## Table of contents
1. [Introduction to Evaluation Metrics](#introduction)
2. [Types of Metric Scores](#types_of_metric_scores)
    - [Statistical Scorers](#statistical_scorers)
    - [LLM-based Scorers](#llm-based-scorers)
    - [Combining Statistical and LLM-based scorers](#combining-statistical-and-llm-based-scorers)
3. [RAG metrics](#rag-metrics)
    - Faithfullness
    - Answer Relevancy
    - Contextual Precision
    - Contextual Recall
    - Contextual Relevancy
4. [Custom metrics](#summarization-metrics)
    - Summarization
    - CMS Responsible AI metrics
5. [Conclusion]

### Introduction to Evaluation Metrics <a name="introduction"></a>
Some introduction text

### Types of Metric Scores <a name="types_of_metric_scores"></a>
The first paragraph text


#### Statistical Scorers <a name="statistical_scorers"></a>
This is a sub paragraph for Statistical Scorers

#### LLM-based Scorers <a name="llm-based_scorers"></a>
This is a sub paragraph for LLM-based Scorers

#### Combining Statistical and LLM-based Scorers <a name="statistical-and-llm-based_scorers"></a>
This is a sub paragraph for Statistical Scorers

### RAG Metrics <a name="rag_metrics"></a>
When it comes to measuring RAG systems...
[RAG Evaluation](RAGEvaluation_with_deepeval.ipynb)

### Custom Metrics <a name="custom_metrics"></a>
The second paragraph text

### Conclusion <a name="conclusion"></a>
The second paragraph text